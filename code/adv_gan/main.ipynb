{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"trgcBKCn9EW6","colab_type":"code","colab":{}},"source":["from torchvision import datasets, transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def load_dataset(dataset_name):\n","\n","    if dataset_name == 'mnist':\n","\n","        num_classes = 10\n","        in_channels = 1\n","\n","        train = datasets.MNIST('data', train=True, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    #transforms.Normalize((0.1307,), (0.3081,))\n","                                    #transforms.Normalize((0.5,), (0.5,))\n","                                    ]))\n","\n","\n","        test = datasets.MNIST('data', train=False, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    #transforms.Normalize((0.1307,), (0.3081,))\n","                                    #transforms.Normalize((0.5,), (0.5,))\n","                                    ]))\n","\n","\n","    elif dataset_name == 'fmnist':\n","\n","        num_classes = 10\n","        in_channels = 1\n","\n","        train = datasets.FashionMNIST('data', train=True, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    # transforms.Normalize((0.1307,), (0.3081,))\n","                                    ]))\n","\n","        test = datasets.FashionMNIST('data', train=False, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    # transforms.Normalize((0.1307,), (0.3081,))\n","                                    ]))\n","\n","\n","    elif dataset_name == 'cifar10':\n","\n","        num_classes = 10\n","        in_channels = 3\n","\n","        train = datasets.CIFAR10('data', train=True, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.RandomHorizontalFlip(),\n","                                    transforms.RandomCrop(32, padding=4),\n","                                    transforms.ToTensor(),\n","                                    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2615))\n","                                    ]))\n","\n","        test = datasets.CIFAR10('data', train=False, download=True,\n","                                transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","                                    ]))\n","\n","\n","    else:\n","        raise Exception(\"dataset must be one of mnist, fmnist and cifar10\")\n","\n","    return train, test, in_channels, num_classes\n","\n","\n","\n","class Model_MNIST(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Model_MNIST, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","\n","        self.conv1_1 = nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.fc1 = nn.Linear(7*7*64, 200)\n","        self.fc2 = nn.Linear(200, self.num_classes)\n","\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","\n","        x = self.maxpool1(x)\n","\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","\n","        x = self.maxpool2(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","class Model_CIFAR(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        super(Model_CIFAR, self).__init__()\n","\n","        self.in_channels = in_channels\n","        self.num_classes = num_classes\n","\n","        self.conv1_1 = nn.Conv2d(self.in_channels, 32, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.conv2_1 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n","\n","        self.fc1 = nn.Linear(3*4*4*64, 200)\n","        self.fc2 = nn.Linear(200, self.num_classes)\n","\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","\n","        x = self.maxpool1(x)\n","\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","\n","        x = self.maxpool2(x)\n","\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","\n","        x = self.maxpool3(x)\n","\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n","        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n","\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n","        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","\n","        residual = x\n","\n","        out = self.relu(self.in1(self.conv1(x)))\n","        out = self.in2(self.conv2(out))\n","\n","        out = out + residual\n","\n","        return out\n","\n","\n","class UpsampleConvLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n","        super(UpsampleConvLayer, self).__init__()\n","\n","        self.upsample = upsample\n","        if upsample:\n","            self.upsample_layer = nn.Upsample(mode='nearest', scale_factor=upsample)\n","\n","        padding = kernel_size // 2\n","\n","        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding)\n","\n","    def forward(self, x):\n","\n","        if self.upsample:\n","            x = self.upsample_layer(x)\n","\n","        x = self.conv2d(x)\n","\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, dataset_name):\n","        super(Generator, self).__init__()\n","        self.dataset_name = dataset_name\n","\n","        if dataset_name in ['mnist', 'fmnist']:\n","            channels = 1\n","        elif dataset_name == 'cifar10':\n","            channels = 3\n","        else:\n","            raise Exception('dataset must be one of mnist, fmnist and cifar10')\n","\n","        self.conv1 = nn.Conv2d(channels, 8, kernel_size=3, stride=1, padding=1)\n","        self.in1 = nn.InstanceNorm2d(8)\n","\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1)\n","        self.in2 = nn.InstanceNorm2d(16)\n","\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n","        self.in3 = nn.InstanceNorm2d(32)\n","\n","        self.resblock1 = ResidualBlock(32)\n","        self.resblock2 = ResidualBlock(32)\n","        self.resblock3 = ResidualBlock(32)\n","        self.resblock4 = ResidualBlock(32)\n","\n","\n","        self.up1 = UpsampleConvLayer(32, 16, kernel_size=3, stride=1, upsample=2)\n","        self.in4 = nn.InstanceNorm2d(16)\n","        self.up2 = UpsampleConvLayer(16, 8, kernel_size=3, stride=1, upsample=2)\n","        self.in5 = nn.InstanceNorm2d(8)\n","\n","\n","        self.conv4 = nn.Conv2d(8, channels, kernel_size=3, stride=1, padding=1)\n","        self.in6 = nn.InstanceNorm2d(channels)\n","\n","\n","    def forward(self, x):\n","\n","        x = F.relu(self.in1(self.conv1(x)))\n","        x = F.relu(self.in2(self.conv2(x)))\n","        x = F.relu(self.in3(self.conv3(x)))\n","\n","        x = self.resblock1(x)\n","        x = self.resblock2(x)\n","        x = self.resblock3(x)\n","        x = self.resblock4(x)\n","\n","        x = F.relu(self.in4(self.up1(x)))\n","        x = F.relu(self.in5(self.up2(x)))\n","\n","        x = self.in6(self.conv4(x)) # remove relu for better performance and when input is [-1 1]\n","\n","        return x\n","\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, dataset_name):\n","        super(Discriminator, self).__init__()\n","        self.dataset_name = dataset_name\n","\n","        if dataset_name in ['mnist', 'fmnist']:\n","            self.conv1 = nn.Conv2d(1, 8, kernel_size=4, stride=2, padding=1)\n","            self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1)\n","            self.in1 = nn.InstanceNorm2d(16)\n","            self.conv3 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1)\n","            self.in2 = nn.InstanceNorm2d(32)\n","            self.fc = nn.Linear(3 * 3 * 32, 1)\n","\n","        elif dataset_name == 'cifar10':\n","            self.conv1 = nn.Conv2d(3, 8, kernel_size=4, stride=2, padding=1)\n","            self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1)\n","            self.in1 = nn.InstanceNorm2d(16)\n","            self.conv3 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1)\n","            self.conv4 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1)\n","            self.in2 = nn.InstanceNorm2d(32)\n","            self.fc = nn.Linear(2 * 2 * 3 * 32, 1)\n","\n","        else:\n","            raise Exception(\"dataset must be one of mnist, fmnist and cifar10\")\n","\n","    def forward(self, x):\n","        if self.dataset_name in ['mnist', 'fmnist']:\n","            x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n","            x = F.leaky_relu(self.in1(self.conv2(x)), negative_slope=0.2)\n","            x = F.leaky_relu(self.in2(self.conv3(x)), negative_slope=0.2)\n","            x = x.view(x.size(0), -1)\n","            x = self.fc(x)\n","        else:\n","            x = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n","            x = F.leaky_relu(self.in1(self.conv2(x)), negative_slope=0.2)\n","            x = F.leaky_relu(self.conv3(x), negative_slope=0.2)\n","            x = F.leaky_relu(self.in2(self.conv4(x)), negative_slope=0.2)\n","            x = x.view(x.size(0), -1)\n","            x = self.fc(x)\n","\n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZWXvqlW9EW9","colab_type":"code","colab":{},"outputId":"80f8c148-8e82-49b4-aad0-8c317515e057"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import DataLoader\n","import datetime\n","import os\n","\n","torch.backends.cudnn.benchmark = True\n","\n","\n","def CWLoss(logits, target, is_targeted, num_classes=10, kappa=0):\n","    # inputs to the softmax function are called logits.\n","    # https://arxiv.org/pdf/1608.04644.pdf\n","    target_one_hot = torch.eye(num_classes).type(logits.type())[target.long()]\n","\n","    # workaround here.\n","    # subtract large value from target class to find other max value\n","    # https://github.com/carlini/nn_robust_attacks/blob/master/l2_attack.py\n","    real = torch.sum(target_one_hot*logits, 1)\n","    other = torch.max((-target_one_hot + 1)*logits - target_one_hot*10000, 1)[0]\n","    kappa = torch.zeros_like(other).fill_(kappa)\n","\n","    if is_targeted:\n","        return torch.sum(torch.max(other-real, kappa))\n","    return torch.sum(torch.max(real-other, kappa))\n","\n","\n","def train(G, D, f, target, is_targeted, thres, criterion_adv, criterion_gan, alpha, beta, train_loader, optimizer_G, optimizer_D, epoch, epochs, device, num_steps=3, verbose=True):\n","    n = 0\n","    acc = 0\n","\n","    G.train()\n","    D.train()\n","    for i, (img, label) in enumerate(train_loader):\n","        valid = torch.ones(img.size(0), 1, requires_grad=False).to(device)\n","        fake = torch.zeros(img.size(0), 1, requires_grad=False).to(device)\n","        img_real = img.to(device)\n","\n","        optimizer_G.zero_grad()\n","\n","        pert = torch.clamp(G(img_real), -thres, thres)\n","        img_fake = pert + img_real\n","        img_fake = img_fake.clamp(min=0, max=1)\n","\n","        y_pred = f(img_fake)\n","\n","        if is_targeted:\n","            y_target = torch.empty_like(label).fill_(target).to(device)\n","            loss_adv = criterion_adv(y_pred, y_target, is_targeted)\n","            acc += torch.sum(torch.max(y_pred, 1)[1] == y_target).item()\n","        else:\n","            y_true = label.to(device)\n","            loss_adv = criterion_adv(y_pred, y_true, is_targeted)\n","            acc += torch.sum(torch.max(y_pred, 1)[1] != y_true).item()\n","\n","        loss_gan = criterion_gan(D(img_fake), valid)\n","        loss_hinge = torch.mean(torch.max(torch.zeros(1, ).type(y_pred.type()), torch.norm(pert.view(pert.size(0), -1), p=2, dim=1) - thres))\n","\n","        loss_g = loss_adv + alpha*loss_gan + beta*loss_hinge\n","\n","        loss_g.backward()\n","        optimizer_G.step()\n","\n","        optimizer_D.zero_grad()\n","        if i % num_steps == 0:\n","            loss_real = criterion_gan(D(img_real), valid)\n","            loss_fake = criterion_gan(D(img_fake.detach()), fake)\n","\n","            loss_d = 0.5*loss_real + 0.5*loss_fake\n","\n","            loss_d.backward()\n","            optimizer_D.step()\n","\n","        n += img.size(0)\n","\n","        if verbose:\n","            print(\"\\rEpoch [%d/%d]: [%d/%d], D Loss: %1.4f, G Loss: %3.4f [H %3.4f, A %3.4f], Acc: %.4f\"\n","                  %(epoch+1, epochs, i, len(train_loader), loss_d.mean().item(), loss_g.mean().item(),\n","                  loss_hinge.mean().item(), loss_adv.mean().item(), acc/n) , end=\"\")\n","    \n","    if verbose: print()\n","    return acc/n\n","\n","\n","def test(G, f, target, is_targeted, thres, test_loader, epoch, epochs, device, verbose=True):\n","    n = 0\n","    acc = 0\n","\n","    G.eval()\n","    for i, (img, label) in enumerate(test_loader):\n","        img_real = img.to(device)\n","\n","        pert = torch.clamp(G(img_real), -thres, thres)\n","        img_fake = pert + img_real\n","        img_fake = img_fake.clamp(min=0, max=1)\n","\n","        y_pred = f(img_fake)\n","\n","        if is_targeted:\n","            y_target = torch.empty_like(label).fill_(target).to(device)\n","            acc += torch.sum(torch.max(y_pred, 1)[1] == y_target).item()\n","        else:\n","            y_true = label.to(device)\n","            acc += torch.sum(torch.max(y_pred, 1)[1] != y_true).item()\n","\n","        n += img.size(0)\n","\n","        if verbose:\n","            print('\\rTest [%d/%d]: [%d/%d]' %(epoch+1, epochs, i, len(test_loader)), end=\"\")\n","    \n","    if verbose: print()\n","    return acc/n\n","\n","\n","lr = 0.001  # learning rate\n","batch_size = 128\n","num_workers = 4  # number of cpu cores that can be used\n","epochs = 20\n","model_name = \"Model_MNIST\"  # must be \"Model_MNIST\", \"Model_FMNIST\" or \"Model_CIFAR\"\n","dataset_name = \"mnist\"  # must be \"mnist\", \"fmnist\", or \"cifar10\"\n","target = -1  # Target label. -1 means untargeted.\n","thres = 0.2 # perturbation bound, used in loss_hinge. 0.2 and 0.3 work the best\n","gpu = True  # use gpu for training\n","\n","device = 'cuda' if gpu else 'cpu'\n","# print(torch.cuda.get_device_name(0))\n","torch.cuda.set_device(0)\n","\n","is_targeted = False\n","if target in range(0, 10):\n","    is_targeted = True # bool variable to indicate targeted or untargeted attack\n","\n","print('Training AdvGAN ', '(Target %d)'%(target) if is_targeted else '(Untargeted)')\n","\n","train_data, test_data, in_channels, num_classes = load_dataset(dataset_name)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","D = Discriminator(dataset_name).to(device)\n","G = Generator(dataset_name).to(device)\n","f = eval(model_name)(in_channels, num_classes).to(device)\n","\n","# load a pre-trained target model\n","checkpoint_path = os.path.join('saved', 'target_models', 'best_%s.pth.tar'%(model_name))\n","checkpoint = torch.load(checkpoint_path, map_location='cpu')\n","f.load_state_dict(checkpoint[\"state_dict\"])\n","f.eval()\n","\n","optimizer_G = optim.Adam(G.parameters(), lr=lr)\n","optimizer_D = optim.Adam(D.parameters(), lr=lr)\n","\n","scheduler_G = StepLR(optimizer_G, step_size=5, gamma=0.1)\n","scheduler_D = StepLR(optimizer_D, step_size=5, gamma=0.1)\n","\n","criterion_adv = CWLoss # loss for fooling target model\n","criterion_gan = nn.MSELoss() # for gan loss\n","alpha = 1 # gan loss multiplication factor\n","beta = 1 # for hinge loss\n","num_steps = 3 # number of generator updates for 1 discriminator update\n","\n","for epoch in range(epochs):\n","    start_time = datetime.datetime.now()\n","\n","    acc_train = train(G, D, f, target, is_targeted, thres, criterion_adv, criterion_gan, alpha, beta, train_loader, optimizer_G, optimizer_D, epoch, epochs, device, num_steps, verbose=True)\n","    acc_test = test(G, f, target, is_targeted, thres, test_loader, epoch, epochs, device, verbose=True)\n","\n","    scheduler_G.step(epoch)\n","    scheduler_D.step(epoch)\n","\n","    end_time = datetime.datetime.now()\n","\n","    print('Epoch [%d/%d]: %.2f seconds\\t'%(epoch+1, epochs, (end_time - start_time).total_seconds()))\n","    print('Train Acc: %.5f\\t'%(acc_train))\n","    print('Test Acc: %.5f\\n'%(acc_test))\n","\n","    torch.save({\"epoch\": epoch+1,\n","                \"epochs\": epochs,\n","                \"is_targeted\": is_targeted,\n","                \"target\": target,\n","                \"thres\": thres,\n","                \"state_dict\": G.state_dict(),\n","                \"acc_test\": acc_test,\n","                \"optimizer\": optimizer_G.state_dict()\n","                }, \"saved/%s_%s.pth.tar\"%(model_name, 'target_%d'%(target) if is_targeted else 'untargeted'))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training AdvGAN  (Untargeted)\n","Epoch [1/20]: [28/469], D Loss: 0.2722, G Loss: 457.8306 [H 5.0013, A 452.0983], Acc: 0.18132"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-7-31bc3abbb5a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_targeted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_gan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_G\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[0macc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_targeted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-7-31bc3abbb5a2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(G, D, f, target, is_targeted, thres, criterion_adv, criterion_gan, alpha, beta, train_loader, optimizer_G, optimizer_D, epoch, epochs, device, num_steps, verbose)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mloss_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_adv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_gan\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_hinge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mloss_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"2Snb1q2-9EXB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}